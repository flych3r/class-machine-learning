{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596136949434",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_line(X, y, lr): \n",
    "    # plotting the actual points as scatter plot \n",
    "    plt.scatter(\n",
    "        X[:, 0], y, color = \"m\", \n",
    "        marker = \"o\", s = 30\n",
    "    ) \n",
    "  \n",
    "    # predicted response vector \n",
    "    y_pred = lr.predict(X)\n",
    "    y_pred = y_pred.flatten()\n",
    "    # plotting the regression line \n",
    "    plt.plot(X[:, 0], y_pred, color = \"g\") \n",
    "  \n",
    "    # putting labels \n",
    "    plt.xlabel('X') \n",
    "    plt.ylabel('y') \n",
    "  \n",
    "    # function to show plot \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(penalty, W):\n",
    "    return penalty * (np.sum(W*W) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_pred):\n",
    "    SS_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    SS_res = np.sum(np.square(y_true - y_pred))\n",
    "    return 1 - SS_res / SS_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, regularization_penalty=0):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.penalty = 0\n",
    "\n",
    "    def fit(self, X, y, method='analytic', **kwargs):\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if method[0] == 'a':\n",
    "            self._analytic_method(X, y)\n",
    "        elif method[0] == 'g':\n",
    "            np.random.seed(42)\n",
    "            self.W = np.random.normal(loc=0, scale=0.1, size=(X.shape[1]))\n",
    "            self.b = 0\n",
    "            self._gradient_descent_runner(X, y, **kwargs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (np.dot(X, self.W) + self.b).reshape(-1, 1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError:\n",
    "            y = y.reshape(-1, 1)\n",
    "        return self._compute_error(X, y)\n",
    "\n",
    "    def _analytic_method(self, X, y):\n",
    "        y_ = np.mean(y, axis=0)\n",
    "        X_ = np.mean(X, axis=0)\n",
    "\n",
    "        SS_xy = np.sum((X - X_) * (y - y_), axis=0)\n",
    "        SS_xx = np.sum(np.square(X - X_), axis=0)\n",
    "        \n",
    "        self.W = SS_xy / SS_xx\n",
    "        self.b = (y_ - np.dot(X_, self.W))\n",
    "\n",
    "    def _compute_error(self, X, y):\n",
    "        N = len(y)\n",
    "        y_hat = (np.dot(X, self.W) + self.b).reshape(-1, 1)\n",
    "        totalError = (1/N) * np.sum(np.square(y - y_hat))\n",
    "        return totalError\n",
    "    \n",
    "    def _step_gradient(self, X, y, learning_rate, batch_size):\n",
    "        N = len(y)\n",
    "        if batch_size is None:\n",
    "            batch_size = N\n",
    "        batch_size = min(batch_size, N)\n",
    "\n",
    "        for b in range(0, N, batch_size):\n",
    "            X_batch = X[b:b + batch_size]\n",
    "            y_batch = y[b:b + batch_size]\n",
    "            y_hat = (np.dot(X_batch, self.W) + self.b).reshape(-1, 1)\n",
    "\n",
    "            dW = -2/N * np.sum(X_batch * (y_batch - y_hat), axis=0)\n",
    "            dW -= l2_penalty(self.penalty, self.W)\n",
    "            db = -2/N * np.sum(y_batch - y_hat)\n",
    "            db -= l2_penalty(self.penalty, self.b)\n",
    "\n",
    "            self.W -= (dW * learning_rate) / max(1, batch_size % N)\n",
    "            self.b -= (db * learning_rate) / max(1, batch_size % N)\n",
    "\n",
    "    def _gradient_descent_runner(\n",
    "        self, X, y, learning_rate=0.1, batch_size=None, epochs=10, tool=None, verbose=False\n",
    "    ):\n",
    "        old_error = self._compute_error(X, y)\n",
    "        for e in range(epochs):\n",
    "            self._step_gradient(X, y, learning_rate, batch_size)\n",
    "            error = self._compute_error(X, y)\n",
    "            if tool and abs(old_error - error) < tool:\n",
    "                break\n",
    "            old_error = error\n",
    "            if verbose:\n",
    "                print('epoch {}: loss = {}'.format(e + 1, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialLinearRegression(LinearRegression):\n",
    "    def __init__(self, p, regularization_penalty=0):\n",
    "        super(PolynomialLinearRegression, self).__init__(regularization_penalty)\n",
    "        self.p = p\n",
    "    \n",
    "    def fit(self, X, y, method='analytic', **kwargs):\n",
    "        X = np.concatenate([np.power(X, i) for i in range(1, self.p + 1)], axis=1)\n",
    "        super().fit(X, y, method, **kwargs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([np.power(X, i) for i in range(1, self.p + 1)], axis=1)\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/trab1_advertising.csv', index_col=0)\n",
    "\n",
    "features = df.drop(columns='sales')\n",
    "features = (features - features.mean()) / features.std()\n",
    "labels = df['sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features.values, \n",
    "    labels.values, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train my_lr -> r2: -319.4278096721214, mse: 57.20771049389113\nTrain sk_lr -> r2: 0.8966445527601498, mse: 2.767891078046973\n\nTest my_lr -> r2: -110.67062895770702, mse: 60.40627026525228\nTest sk_lr -> r2: 0.8935163320163658, mse: 2.880023730094191\n"
    }
   ],
   "source": [
    "my_lr = LinearRegression(regularization_penalty=0)\n",
    "my_lr.fit(X_train, y_train, method='a', learning_rate=0.1, epochs=10)\n",
    "\n",
    "sk_lr = LR()\n",
    "sk_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = my_lr.predict(X_train)\n",
    "print('Train my_lr -> r2: {}, mse: {}'.format(\n",
    "    r2_score(y_train, y_pred),\n",
    "    mse(y_train, y_pred),\n",
    "))\n",
    "y_pred = sk_lr.predict(X_train)\n",
    "print('Train sk_lr -> r2: {}, mse: {}'.format(\n",
    "    r2_score(y_train, y_pred),\n",
    "    mse(y_train, y_pred),\n",
    "))\n",
    "print()\n",
    "y_pred = my_lr.predict(X_test)\n",
    "print('Test my_lr -> r2: {}, mse: {}'.format(\n",
    "    r2_score(y_test, y_pred),\n",
    "    mse(y_test, y_pred),\n",
    "))\n",
    "y_pred = sk_lr.predict(X_test)\n",
    "print('Test sk_lr -> r2: {}, mse: {}'.format(\n",
    "    r2_score(y_test, y_pred),\n",
    "    mse(y_test, y_pred),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}